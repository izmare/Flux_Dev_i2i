{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/izmare/Flux_Dev_i2i/blob/main/FluxDevi2i.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup and Installation\n",
        "#@markdown This cell clones the repository, sets up the environment, and installs necessary dependencies.\n",
        "\n",
        "# Clone the repository and set the working directory\n",
        "%cd /content\n",
        "!git clone -b totoro3 https://github.com/camenduru/ComfyUI /content/TotoroUI\n",
        "%cd /content/TotoroUI\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.27.post2\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "# Create directories for models\n",
        "import os\n",
        "unet_dir = \"/content/TotoroUI/models/unet/\"\n",
        "vae_dir = \"/content/TotoroUI/models/vae/\"\n",
        "clip_dir = \"/content/TotoroUI/models/clip/\"\n",
        "os.makedirs(unet_dir, exist_ok=True)\n",
        "os.makedirs(vae_dir, exist_ok=True)\n",
        "os.makedirs(clip_dir, exist_ok=True)\n",
        "\n",
        "# Download model files\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/flux1-dev-fp8.safetensors -d /content/TotoroUI/models/unet -o flux1-dev-fp8.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/ae.sft -d /content/TotoroUI/models/vae -o ae.sft\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/clip_l.safetensors -d /content/TotoroUI/models/clip -o clip_l.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/t5xxl_fp8_e4m3fn.safetensors -d /content/TotoroUI/models/clip -o t5xxl_fp8_e4m3fn.safetensors\n",
        "\n",
        "print(\"Setup completed.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gw3UAyvcVSrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VjYy0F2gZIPR",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "487b4996-6f74-4a0c-9fca-144da8aa450a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model setup completed.\n"
          ]
        }
      ],
      "source": [
        "#@title Model Setup and Installation\n",
        "#@markdown This cell sets up the environment and downloads selected models based on the checkboxes.\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Directories for model storage\n",
        "unet_dir = \"/content/TotoroUI/models/unet/\"\n",
        "vae_dir = \"/content/TotoroUI/models/vae/\"\n",
        "clip_dir = \"/content/TotoroUI/models/clip/\"\n",
        "\n",
        "# URLs for model files\n",
        "urls = {\n",
        "    \"flux_dev\": {\n",
        "        \"unet\": \"https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/flux1-dev-fp8.safetensors\",\n",
        "        \"vae\": \"https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/ae.sft\",\n",
        "        \"clip\": [\n",
        "            \"https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/clip_l.safetensors\",\n",
        "            \"https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/t5xxl_fp8_e4m3fn.safetensors\"\n",
        "        ]\n",
        "    },\n",
        "    \"flux_schnell\": {\n",
        "        \"unet\": \"https://huggingface.co/camenduru/FLUX.1-schnell/resolve/main/flux1-schnell-fp8.safetensors\",\n",
        "        \"vae\": \"https://huggingface.co/camenduru/FLUX.1-schnell/resolve/main/ae.sft\",\n",
        "        \"clip\": [\n",
        "            \"https://huggingface.co/camenduru/FLUX.1-schnell/resolve/main/clip_l.safetensors\",\n",
        "            \"https://huggingface.co/camenduru/FLUX.1-schnell/resolve/main/t5xxl_fp8_e4m3fn.safetensors\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Checkbox options for model downloads\n",
        "download_flux_dev = True  #@param {type:\"boolean\"}\n",
        "download_flux_schnell = False  #@param {type:\"boolean\"}\n",
        "\n",
        "# Function to download files with a progress bar\n",
        "def download_file(url, output_path):\n",
        "    if not os.path.exists(output_path):\n",
        "        with tqdm(total=100, desc=f\"Downloading {os.path.basename(output_path)}\", unit=\"B\", unit_scale=True) as pbar:\n",
        "            def reporthook(block_num, block_size, total_size):\n",
        "                pbar.update(block_num * block_size / total_size * 100)\n",
        "            urllib.request.urlretrieve(url, output_path, reporthook=reporthook)\n",
        "\n",
        "# Download selected model files\n",
        "if download_flux_dev:\n",
        "    if not os.path.exists(os.path.join(unet_dir, \"flux1-dev-fp8.safetensors\")):\n",
        "        download_file(urls[\"flux_dev\"][\"unet\"], os.path.join(unet_dir, \"flux1-dev-fp8.safetensors\"))\n",
        "    if not os.path.exists(os.path.join(vae_dir, \"ae.sft\")):\n",
        "        download_file(urls[\"flux_dev\"][\"vae\"], os.path.join(vae_dir, \"ae.sft\"))\n",
        "    for clip_url in urls[\"flux_dev\"][\"clip\"]:\n",
        "        if not os.path.exists(os.path.join(clip_dir, os.path.basename(clip_url))):\n",
        "            download_file(clip_url, os.path.join(clip_dir, os.path.basename(clip_url)))\n",
        "\n",
        "if download_flux_schnell:\n",
        "    if not os.path.exists(os.path.join(unet_dir, \"flux1-schnell-fp8.safetensors\")):\n",
        "        download_file(urls[\"flux_schnell\"][\"unet\"], os.path.join(unet_dir, \"flux1-schnell-fp8.safetensors\"))\n",
        "    if not os.path.exists(os.path.join(vae_dir, \"ae.sft\")):\n",
        "        download_file(urls[\"flux_schnell\"][\"vae\"], os.path.join(vae_dir, \"ae.sft\"))\n",
        "    for clip_url in urls[\"flux_schnell\"][\"clip\"]:\n",
        "        if not os.path.exists(os.path.join(clip_dir, os.path.basename(clip_url))):\n",
        "            download_file(clip_url, os.path.join(clip_dir, os.path.basename(clip_url)))\n",
        "\n",
        "print(\"Model setup completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Anime Style Image Generation\n",
        "#@markdown This code generates an image in an anime style using the selected model. The output file name is incrementally generated to prevent overwriting.\n",
        "\n",
        "import random\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import nodes\n",
        "from nodes import NODE_CLASS_MAPPINGS\n",
        "from totoro_extras import nodes_custom_sampler\n",
        "from totoro_extras import nodes_post_processing\n",
        "from totoro import model_management\n",
        "\n",
        "# Import modules from the repository\n",
        "import sys\n",
        "sys.path.append('/content/TotoroUI')\n",
        "\n",
        "# Load classes from nodes\n",
        "DualCLIPLoader = NODE_CLASS_MAPPINGS[\"DualCLIPLoader\"]()\n",
        "UNETLoader = NODE_CLASS_MAPPINGS[\"UNETLoader\"]()\n",
        "RandomNoise = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"RandomNoise\"]()\n",
        "BasicGuider = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicGuider\"]()\n",
        "KSamplerSelect = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"KSamplerSelect\"]()\n",
        "BasicScheduler = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicScheduler\"]()\n",
        "SamplerCustomAdvanced = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
        "VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
        "VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
        "VAEEncode = NODE_CLASS_MAPPINGS[\"VAEEncode\"]()\n",
        "EmptyLatentImage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
        "ImageScaleToTotalPixels = nodes_post_processing.NODE_CLASS_MAPPINGS[\"ImageScaleToTotalPixels\"]()\n",
        "\n",
        "# Generate an incremental file name\n",
        "def generate_incremental_filename(base_dir=\"/content/\", base_name=\"image\", extension=\".png\"):\n",
        "    i = 0\n",
        "    while os.path.exists(os.path.join(base_dir, f\"{base_name}_{i:04d}{extension}\")):\n",
        "        i += 1\n",
        "    return os.path.join(base_dir, f\"{base_name}_{i:04d}{extension}\")\n",
        "\n",
        "output_filename = generate_incremental_filename()\n",
        "\n",
        "# Input fields\n",
        "model_choice = \"flux_dev\"  #@param [\"flux_dev\", \"flux_schnell\"]\n",
        "positive_prompt = \"Pixel art anime girl, short white hair with short twintails, cat ears headband, red eyes, cheerful expression, black and red outfit, crop top, short skirt, belt with buckle, thigh-high striped stockings, boots with gold tips, dual flamethrower weapons, gold and black design, backpack fuel tank, 16-bit graphics, retro gaming aesthetic, limited color palette, chunky pixels, no anti-aliasing, sharp pixel edges, low-resolution sprite, SNES era style, 2D side-scrolling game character, simplified pixel details, blocky shapes, pixelated shading, classic arcade look, 8-bit inspired, old-school video game character, square pixels only, mosaic-like appearance, pixel-perfect edges, retro pixel art style, low color depth, nostalgic graphics, dithering effects, dynamic action pose, contrast between light hair and dark outfit\"  #@param {type:\"string\"}\n",
        "width = 512  #@param {type:\"slider\", min:256, max:2048, step:64}\n",
        "height = 1024  #@param {type:\"slider\", min:256, max:2048, step:64}\n",
        "seed = 0  #@param {type:\"number\"}\n",
        "steps = 20  #@param {type:\"slider\", min:10, max:100, step:5}\n",
        "sampler_name = \"euler\"  #@param [\"euler\", \"ddim\", \"plms\", \"heun\"]\n",
        "scheduler = \"simple\"  #@param [\"simple\", \"linear\", \"cosine\"]\n",
        "guidance_scale = 7.5  #@param {type:\"slider\", min:1.0, max:20.0, step:0.5}\n",
        "image_dir = \"/content/IconRole24_webp_resized.png\"  #@param {type:\"string\"}\n",
        "\n",
        "# Convert input image to PNG if necessary\n",
        "image_ext = os.path.splitext(image_dir)[-1].lower()\n",
        "if image_ext not in [\".png\"]:\n",
        "    img = Image.open(image_dir)\n",
        "    image_dir = os.path.splitext(image_dir)[0] + \".png\"\n",
        "    img.save(image_dir)\n",
        "\n",
        "# Load models based on user choice\n",
        "with torch.inference_mode():\n",
        "    if model_choice == \"flux_dev\":\n",
        "        clip = DualCLIPLoader.load_clip(\"t5xxl_fp8_e4m3fn.safetensors\", \"clip_l.safetensors\", \"flux\")[0]\n",
        "        unet = UNETLoader.load_unet(\"flux1-dev-fp8.safetensors\", \"fp8_e4m3fn\")[0]\n",
        "        vae = VAELoader.load_vae(\"ae.sft\")[0]\n",
        "    elif model_choice == \"flux_schnell\":\n",
        "        clip = DualCLIPLoader.load_clip(\"t5xxl_fp8_e4m3fn.safetensors\", \"clip_l.safetensors\", \"flux\")[0]\n",
        "        unet = UNETLoader.load_unet(\"flux1-schnell-fp8.safetensors\", \"fp8_e4m3fn\")[0]\n",
        "        vae = VAELoader.load_vae(\"ae.sft\")[0]\n",
        "\n",
        "    if seed == 0:\n",
        "        seed = random.randint(0, 18446744073709551615)\n",
        "    print(f\"Seed: {seed}\")\n",
        "\n",
        "    cond, pooled = clip.encode_from_tokens(clip.tokenize(positive_prompt), return_pooled=True)\n",
        "    cond = [[cond, {\"pooled_output\": pooled}]]\n",
        "    noise = RandomNoise.get_noise(seed)[0]\n",
        "    guider = BasicGuider.get_guider(unet, cond, guidance_scale=guidance_scale)[0]\n",
        "    sampler = KSamplerSelect.get_sampler(sampler_name)[0]\n",
        "    sigmas = BasicScheduler.get_sigmas(unet, scheduler, steps, 0.75)[0]\n",
        "\n",
        "    image = nodes.LoadImage().load_image(image_dir)[0]\n",
        "    latent_image = ImageScaleToTotalPixels.upscale(image, \"lanczos\", 1.0)[0]\n",
        "    latent_image = VAEEncode.encode(vae, latent_image)[0]\n",
        "\n",
        "    sample, sample_denoised = SamplerCustomAdvanced.sample(noise, guider, sampler, sigmas, latent_image)\n",
        "    model_management.soft_empty_cache()\n",
        "    decoded = VAEDecode.decode(vae, sample)[0].detach()\n",
        "    Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0]).save(output_filename)\n",
        "\n",
        "Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0])\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wouwe7HxeKG_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "076973cd-20aa-447e-9d7e-8979ff31aa2f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'totoro_extras'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1b70f39c436c>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtotoro_extras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnodes_custom_sampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtotoro_extras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnodes_post_processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtotoro\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_management\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'totoro_extras'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}